---
# Supported annotations:- 
# https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/service/annotations/
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: lb-demo
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip  # use ip modefor routing the traffic as it goes straight to pod IPs because NLB registers Pod IPs directly as targets instead of instance mode to routing your request through node ports then kube-proxy then pods 
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing  # don't use internet facing for internal services such as dashboards prometheus and grafana
    # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
spec:
  type: LoadBalancer
  ports:
    - port: 8080
      targetPort: http
  selector:
    app: myapp




# Quick summary

# Those annotations tell the AWS Load Balancer Controller (LBC) how to create an AWS Network Load Balancer (NLB) for this Service and how to route traffic to your pods (register pod IPs vs register nodes). The controller reads those annotations and then creates/configures the AWS LB and target groups accordingly. 
# Kubernetes SIGs
# +1

# Your annotations — explained
# service.beta.kubernetes.io/aws-load-balancer-type: external

# What it does: tells the cluster to let the AWS Load Balancer Controller (instead of the built-in cloud-provider controller) reconcile this Service and create the AWS LB. The controller supports values like external (recommended) and historically nlb-ip (kept for backward compatibility). 
# Kubernetes SIGs
# +1

# Why you use it: it ensures the LBC (the modern controller) handles the Service, so you get NLB features like IP-target mode and LBC-specific annotations. Don’t change this after the Service is created — you should recreate the Service to change this safely. 
# Kubernetes SIGs

# service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip

# What it does: chooses how the NLB registers backends:

# ip → the NLB registers pod IPs directly as targets (traffic goes straight to pod IPs).

# instance → the NLB registers node instances and traffic hits the node’s NodePort, then kube-proxy forwards to pods. 
# Kubernetes SIGs
# +1

# Practical differences / why pick one:

# ip reduces a network hop (LB → pod) and can preserve network pathing better; it also supports pods on EC2 and Fargate. 
# Kubernetes SIGs
# +1

# instance works with EC2 nodes and is useful if you rely on kube-proxy/node networking; instance does not work for Fargate. 
# Kubernetes SIGs

# Client IP preservation: note that client IP preservation behavior differs — instance mode preserves client IP by default; ip target mode may have different defaults (check the docs for preserve-client-ip behavior for your controller version). 
# Kubernetes SIGs

# service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing

# What it does: chooses internet-facing (public DNS + public IPs) vs internal (only reachable inside your VPC). Valid values: internet-facing or internal. Use internet-facing for public services and do not use it for internal dashboards (Prometheus, Grafana, etc.). 
# Kubernetes SIGs
# +1

# Note: older setups used service.beta.kubernetes.io/aws-load-balancer-internal: "true" — that’s deprecated in favor of aws-load-balancer-scheme. If both exist, aws-load-balancer-scheme takes precedence. 
# Kubernetes SIGs

# # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*" (commented out)

# What it would do if enabled: enable PROXY protocol on the NLB so the original client IP (and port) are injected into the TCP stream for the backend. Useful when you use instance target mode and want the backend to see original client IPs. The canonical value in this annotation is "*". 
# Kubernetes

# Caveat: backends must speak/understand PROXY protocol (web server or sidecar must parse it); if they don’t, enabling it will break your app.

# The rest of your Service manifest (ports / selector)

# spec.type: LoadBalancer + the annotations is what causes the LBC to create an NLB. The ports block maps port: 8080 to targetPort: http — targetPort: http must match a named container port (or a numeric port) in your Deployment. The selector: app: myapp connects the Service to the pod(s). 
# AWS Documentation

# Useful related annotations you may want later

# service.beta.kubernetes.io/aws-load-balancer-ssl-cert — attach an ACM cert ARN for TLS on NLB listeners. 
# Kubernetes

# service.beta.kubernetes.io/load-balancer-source-ranges — restrict allowed CIDRs (whitelist). 
# Kubernetes SIGs

# service.beta.kubernetes.io/aws-load-balancer-healthcheck-* — customize health-check path/port/thresholds. 
# Kubernetes SIGs

# Gotchas & tips (practical)

# Don’t edit these annotations in-place after Service creation — some have no effect when changed; safer: delete + recreate the Service with new annotations. 
# Kubernetes SIGs

# Security: internet-facing exposes the LB publicly. If the service should be internal, use aws-load-balancer-scheme: internal or use load-balancer-source-ranges to restrict CIDRs. 
# Kubernetes SIGs

# Fargate: if you plan to run pods on Fargate, prefer ip target type (Fargate requires IP targets). If you’re EC2-only, both modes work but pick based on whether you want kube-proxy hop or direct pod IPs. 
# Kubernetes SIGs
# +1

# Names & limits: if you set aws-load-balancer-name, adhere to length limits (controller will error on >32 chars). 
# Kubernetes SIGs

# Small checklist for lb-demo

# If you want a public NLB that routes straight to pods (recommended for high performance): aws-load-balancer-type: external, aws-load-balancer-nlb-target-type: ip, aws-load-balancer-scheme: internet-facing. (That matches your current YAML.) 
# Kubernetes SIGs
# +1

# Ensure the AWS Load Balancer Controller is installed and has IAM permissions (you already set that up earlier). 
# AWS Documentation

# If you want I can:

# show you how the corresponding AWS resources will look (example target group / listener), or

# update your YAML to add a health-check or TLS annotation, or

# change the commented proxy-protocol into a working example (with notes on backend config).

# Which one would help next?